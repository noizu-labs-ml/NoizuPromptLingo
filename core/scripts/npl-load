#!/usr/bin/env python3
"""
npl-load - Load NPL components, metadata, and style guides with dependency tracking
"""

import os
import sys
import argparse
import glob
import fnmatch
import re
import yaml
import json
import tempfile
import subprocess
import concurrent.futures
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Set, Tuple, Optional, Dict, Any

# Version comparison helper
def parse_version(version_str: str) -> Tuple[int, ...]:
    """Parse semantic version string to tuple for comparison"""
    try:
        parts = version_str.strip().split('.')
        return tuple(int(p) for p in parts)
    except (ValueError, AttributeError):
        return (0, 0, 0)


@dataclass
class SectionMetadata:
    """Metadata for a versioned prompt section"""
    name: str
    version: str
    content: str = ""
    source_path: Optional[str] = None
    start_pos: int = 0
    end_pos: int = 0

    def version_tuple(self) -> Tuple[int, ...]:
        return parse_version(self.version)

# Resource type configuration - defines search paths and environment variables for each type
RESOURCE_CONFIG = {
    'component': {
        'env_var': 'NPL_HOME',
        'env_suffix': 'npl',  # Append to env var path
        'subdirs': ['npl'],
    },
    'meta': {
        'env_var': 'NPL_META',
        'subdirs': ['meta'],
    },
    'style': {
        'env_var': 'NPL_STYLE_GUIDE',
        'subdirs': ['conventions'],
        'themed': True,  # Uses theme directories
    },
    'schema': {
        'env_var': 'NPL_HOME',
        'env_suffix': 'core/schema',
        'subdirs': ['core/schema'],
        'extra_paths': ['/env/npl/core/schema'],
    },
    'agent': {
        'extra_paths': ['./.claude/agents', '~/.claude/agents'],
        'env_var': 'NPL_HOME',
        'env_suffix': 'core/agents',
        'subdirs': ['core/agents'],
    },
    'spec': {
        'subdirs': ['specifications'],
        'env_var': 'NPL_HOME',
        'env_suffix': 'core/specifications',
        'core_subdirs': ['core/specifications'],
    },
    'persona': {
        'env_var': 'NPL_PERSONA_DIR',
        'subdirs': ['personas'],
    },
    'user-persona': {
        'env_var': 'NPL_USER_PERSONA_DIR',
        'subdirs': ['user-personas'],
    },
    'prd': {
        'subdirs': ['prds'],
    },
    'story': {
        'subdirs': ['user-stories'],
    },
    'prompt': {
        'subdirs': ['prompts'],
        'env_var': 'NPL_HOME',
        'env_suffix': 'core/prompts',
        'core_subdirs': ['core/prompts'],
    },
    'workflow': {
        'env_var': 'NPL_WORKFLOW_DIR',
        'subdirs': ['workflows'],
    },
}


class NPLLoader:
    def __init__(self):
        # Environment variables with fallbacks
        self.npl_home = os.environ.get('NPL_HOME', None)
        self.npl_meta = os.environ.get('NPL_META', None)
        self.npl_style_guide = os.environ.get('NPL_STYLE_GUIDE', None)
        self.npl_theme = os.environ.get('NPL_THEME', 'default')

        # Platform-specific global config (computed once)
        if sys.platform.startswith('win'):
            self.global_npl = Path(os.environ.get('PROGRAMDATA', 'C:\\ProgramData')) / 'npl'
        elif sys.platform == 'darwin':
            self.global_npl = Path('/Library/Application Support/npl')
        else:
            self.global_npl = Path('/etc/npl')

        # Track loaded items
        self.loaded_components = set()
        self.loaded_meta = set()
        self.loaded_style = set()
        self.loaded_agents = set()
        self.loaded_specs = set()
        self.loaded_personas = set()
        self.loaded_user_personas = set()
        self.loaded_prds = set()
        self.loaded_stories = set()
        self.loaded_prompts = set()
        self.loaded_workflows = set()
        
    def _build_search_paths(self, config: Dict[str, Any]) -> List[Path]:
        """Build search paths from resource config"""
        paths = []

        # Add extra paths first (highest priority, e.g., .claude/agents)
        for p in config.get('extra_paths', []):
            expanded = Path(p).expanduser() if '~' in p else Path(p)
            paths.append(expanded)

        # Add environment variable override path
        env_var = config.get('env_var')
        if env_var:
            env_value = os.environ.get(env_var)
            if env_value:
                base = Path(env_value)
                suffix = config.get('env_suffix')
                paths.append(base / suffix if suffix else base)

        # Add standard hierarchy for each subdir
        for subdir in config.get('subdirs', []):
            paths.extend([
                Path('./.npl') / subdir,
                Path.home() / '.npl' / subdir,
                self.global_npl / subdir
            ])

        # Add core subdirs (for types that have separate core paths)
        for core_subdir in config.get('core_subdirs', []):
            paths.extend([
                Path('./.npl') / core_subdir,
                Path.home() / '.npl' / core_subdir,
                self.global_npl / core_subdir
            ])

        return paths

    def get_search_paths(self, resource_type='component') -> List[Path]:
        """Get search paths based on resource type using config-driven approach"""
        config = RESOURCE_CONFIG.get(resource_type)
        if not config:
            return []
        return self._build_search_paths(config)

    def get_npl_init_paths(self):
        """Get search paths for the main npl.md file (one level up from search paths)"""
        paths = []

        # Search paths are one level up from the component search paths
        if self.npl_home:
            paths.append(Path(self.npl_home))
        paths.extend([
            Path('./.npl'),
            Path.home() / '.npl',
            self.global_npl,
            Path('.')  # Also check current directory
        ])

        return paths
        
    def resolve_path(self, item: str, resource_type: str) -> Optional[Tuple[Path, Path]]:
        """Convert dot notation to path and find first existing file, returning (base_dir, file_path)"""
        path_parts = item.split('.')
        relative_path = Path(*path_parts[:-1]) / f"{path_parts[-1]}.md" if len(path_parts) > 1 else Path(f"{item}.md")
        search_paths = self.get_search_paths(resource_type)

        if resource_type == 'style':
            # Try theme-specific first (if not default), then default
            if self.npl_theme and self.npl_theme != 'default':
                for base_path in search_paths:
                    theme_path = base_path / self.npl_theme / relative_path
                    if theme_path.exists():
                        return (base_path / self.npl_theme, theme_path)
            # Fallback to default
            for base_path in search_paths:
                full_path = base_path / 'default' / relative_path
                if full_path.exists():
                    return (base_path / 'default', full_path)
            return None

        # component/meta
        for base_path in search_paths:
            full_path = base_path / relative_path
            if full_path.exists():
                return (base_path, full_path)
        return None

    def load_file_with_patch(self, base: Path, file: Path, source_type: str, item: str) -> str:
        """
        Load content from file and apply patch if available.
        - base: the root directory under which 'file' was found (e.g., .../.npl/npl, .../.npl/meta, .../conventions/<theme>)
        - file: the absolute path to the discovered file
        - source_type: 'component' | 'meta' | 'style'
        - item: the dot-notation name used to look up the file (for labeling)
        """
        try:
            with open(file, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"Error loading {file}: {e}")
            return ""

        # Compute relative path of the file under its discovery base
        try:
            rel_path = file.relative_to(base)
        except Exception:
            # As a fallback, just use the filename
            rel_path = file.name

        # Patch search bases (project, user, system)
        if source_type == 'component':
            patch_bases = [
                Path('./.npl/npl'),
                Path.home() / '.npl/npl',
                Path('/etc/npl/npl')
            ]
        elif source_type == 'meta':
            patch_bases = [
                Path('./.npl/meta'),
                Path.home() / '.npl/meta',
                Path('/etc/npl/meta')
            ]
        elif source_type == 'style':
            patch_bases = [
                Path('./.npl/conventions'),
                Path.home() / '.npl/conventions',
                Path('/etc/npl/conventions')
            ]
        else:
            patch_bases = []

        # Theme-aware search for styles
        candidates: List[Path] = []
        if source_type == 'style':
            # If base is .../conventions/<theme>, include theme segment in rel path for patches
            theme_dir = base.name if base.parent.name == 'conventions' else None
            for b in patch_bases:
                if theme_dir:
                    candidates.append((b / theme_dir / rel_path).with_suffix('.patch.md'))
                    if theme_dir != 'default':
                        candidates.append((b / 'default' / rel_path).with_suffix('.patch.md'))
                # Also allow no-theme patch override
                candidates.append((b / rel_path).with_suffix('.patch.md'))
        else:
            # Components/meta: straight relative placement
            for b in patch_bases:
                candidates.append((b / rel_path).with_suffix('.patch.md'))

        for patch_file in candidates:
            if patch_file.exists():
                try:
                    with open(patch_file, 'r', encoding='utf-8') as pf:
                        patch_content = pf.read()
                    return f"# {item}:\n„Äé(patch)\n{patch_content}\n„Äè\n{content}"
                except Exception as e:
                    print(f"Error loading patch {patch_file}: {e}")

        return f"# {item}:\n{content}"

    def is_skipped(self, item: str, skip_patterns: Set[str]) -> bool:
        """Check if item matches any skip pattern (supports wildcards)"""
        for pattern in skip_patterns:
            if fnmatch.fnmatch(item, pattern):
                return True
        return False

    def _get_tracking_set(self, resource_type: str) -> Set[str]:
        """Get the tracking set for a given resource type"""
        tracking_sets = {
            'component': self.loaded_components,
            'meta': self.loaded_meta,
            'style': self.loaded_style,
            'spec': self.loaded_specs,
            'persona': self.loaded_personas,
            'user-persona': self.loaded_user_personas,
            'prd': self.loaded_prds,
            'story': self.loaded_stories,
            'prompt': self.loaded_prompts,
            'workflow': self.loaded_workflows,
        }
        return tracking_sets.get(resource_type, set())

    def load_resources(self, items: List[str], resource_type: str, skip: Set[str]) -> List[Tuple[str, str]]:
        """Generic resource loader for any resource type"""
        loaded = []
        tracking_set = self._get_tracking_set(resource_type)
        config = RESOURCE_CONFIG.get(resource_type, {})
        is_themed = config.get('themed', False)

        for pattern in items:
            if '*' in pattern:
                base_paths = self.get_search_paths(resource_type)
                for base_path in base_paths:
                    if is_themed:
                        # Theme-aware glob for style resources
                        for theme_dir in [self.npl_theme, 'default']:
                            theme_path = base_path / theme_dir
                            if theme_path.exists():
                                glob_pattern = pattern.replace('.', '/')
                                for match in theme_path.glob(f"{glob_pattern}.md"):
                                    relative = match.relative_to(theme_path)
                                    item = str(relative).replace('/', '.').replace('.md', '')
                                    if not self.is_skipped(item, skip) and item not in tracking_set:
                                        content = self.load_file_with_patch(theme_path, match, resource_type, item)
                                        if content:
                                            loaded.append((item, content))
                                            tracking_set.add(item)
                    else:
                        glob_pattern = pattern.replace('.', '/')
                        for match in base_path.glob(f"{glob_pattern}.md"):
                            relative = match.relative_to(base_path)
                            item = str(relative).replace('/', '.').replace('.md', '')
                            if not self.is_skipped(item, skip) and item not in tracking_set:
                                content = self.load_file_with_patch(base_path, match, resource_type, item)
                                if content:
                                    loaded.append((item, content))
                                    tracking_set.add(item)
            else:
                if not self.is_skipped(pattern, skip) and pattern not in tracking_set:
                    result = self.resolve_path(pattern, resource_type)
                    if result:
                        base_path, file_path = result
                        content = self.load_file_with_patch(base_path, file_path, resource_type, pattern)
                        if content:
                            loaded.append((pattern, content))
                            tracking_set.add(pattern)
        return loaded

    # Backward-compatible wrapper methods
    def load_components(self, components: List[str], skip: Set[str]) -> List[Tuple[str, str]]:
        """Load component files with patch support"""
        return self.load_resources(components, 'component', skip)

    def load_meta(self, items: List[str], skip: Set[str]) -> List[Tuple[str, str]]:
        """Load metadata files with patch support"""
        return self.load_resources(items, 'meta', skip)

    def load_style(self, items: List[str], skip: Set[str]) -> List[Tuple[str, str]]:
        """Load style guide files with patch support"""
        return self.load_resources(items, 'style', skip)

    def load_specs(self, items: List[str], skip: Set[str]) -> List[Tuple[str, str]]:
        """Load specification files"""
        return self.load_resources(items, 'spec', skip)

    def load_personas(self, items: List[str], skip: Set[str]) -> List[Tuple[str, str]]:
        """Load persona files (agent personas)"""
        return self.load_resources(items, 'persona', skip)

    def load_user_personas(self, items: List[str], skip: Set[str]) -> List[Tuple[str, str]]:
        """Load user persona files (for user story planning)"""
        return self.load_resources(items, 'user-persona', skip)

    def load_prds(self, items: List[str], skip: Set[str]) -> List[Tuple[str, str]]:
        """Load PRD document files"""
        return self.load_resources(items, 'prd', skip)

    def load_stories(self, items: List[str], skip: Set[str]) -> List[Tuple[str, str]]:
        """Load user story files"""
        return self.load_resources(items, 'story', skip)

    def load_prompts(self, items: List[str], skip: Set[str]) -> List[Tuple[str, str]]:
        """Load prompt files"""
        return self.load_resources(items, 'prompt', skip)

    def load_workflows(self, items: List[str], skip: Set[str]) -> List[Tuple[str, str]]:
        """Load workflow files"""
        return self.load_resources(items, 'workflow', skip)

    def init_claude(self, target_path: str = './CLAUDE.md',
                    prompts: List[str] = None) -> Tuple[int, int]:
        """
        Append standard prompts to CLAUDE.md file.
        Returns (added_count, skipped_count).
        """
        if prompts is None:
            prompts = ['npl-conventions', 'npl-load-directive', 'npl-scripts', 'sqlite-guide']

        # Read existing CLAUDE.md or create empty
        existing_content = ''
        target = Path(target_path)
        if target.exists():
            try:
                existing_content = target.read_text(encoding='utf-8')
            except Exception as e:
                print(f"Error reading {target_path}: {e}", file=sys.stderr)
                return (0, 0)

        # Parse existing versioned sections
        existing_sections = self.parse_npl_sections(existing_content)

        # Collect prompts to append
        to_append = []
        skipped = 0
        for prompt_name in prompts:
            # Skip if section already exists (by name from YAML header)
            if prompt_name in existing_sections:
                skipped += 1
                print(f"Skipping {prompt_name}: already present in {target_path}", file=sys.stderr)
                continue

            # Resolve prompt path
            result = self.resolve_path(prompt_name, 'prompt')
            if result:
                _, file_path = result
                try:
                    content = file_path.read_text(encoding='utf-8')
                    to_append.append((prompt_name, content))
                except Exception as e:
                    print(f"Error reading prompt {file_path}: {e}", file=sys.stderr)
            else:
                print(f"Prompt not found: {prompt_name}", file=sys.stderr)

        # Append with separator (join-style: separator only between items)
        if to_append:
            try:
                with open(target_path, 'a', encoding='utf-8') as f:
                    for i, (name, content) in enumerate(to_append):
                        # Strip leading/trailing * * * from content to avoid double separators
                        content = _strip_section_separators(content)

                        # Add separator if there's existing content or previous prompts
                        if i > 0 or existing_content.strip():
                            f.write('\n\n* * *\n\n')
                        f.write(content)
                        print(f"Added {name} to {target_path}", file=sys.stderr)
            except Exception as e:
                print(f"Error writing to {target_path}: {e}", file=sys.stderr)
                return (0, skipped)

        return (len(to_append), skipped)

    def parse_npl_sections(self, content: str) -> Dict[str, SectionMetadata]:
        """
        Parse content for npl-instructions YAML blocks.
        Returns dict mapping section name to SectionMetadata.
        """
        sections = {}

        # Pattern to match npl-instructions blocks
        # Format: [* * *\n]npl-instructions:\n   name: ...\n   version: ...\n---
        # The * * * prefix is optional (present in CLAUDE.md, absent in source files)
        pattern = r'(?:\* \* \*\s*\n)?npl-instructions:\s*\n((?:\s+\S.*\n)+)---'

        for match in re.finditer(pattern, content):
            yaml_block = match.group(1)
            start_pos = match.start()

            try:
                # Parse the YAML content
                parsed = yaml.safe_load(yaml_block)
                if parsed and isinstance(parsed, dict):
                    name = parsed.get('name', '')
                    version = parsed.get('version', '0.0.0')

                    if name:
                        # Find end of section (next * * * or EOF)
                        remaining = content[match.end():]
                        next_section = remaining.find('\n* * *\n')
                        if next_section == -1:
                            end_pos = len(content)
                        else:
                            end_pos = match.end() + next_section

                        section_content = content[start_pos:end_pos].strip()
                        sections[name] = SectionMetadata(
                            name=name,
                            version=str(version),
                            content=section_content,
                            start_pos=start_pos,
                            end_pos=end_pos
                        )
            except yaml.YAMLError:
                continue

        return sections

    def discover_prompt_sources(self) -> Dict[str, SectionMetadata]:
        """
        Discover all versioned prompts from source paths.
        Returns dict mapping section name to SectionMetadata with source_path.
        """
        sources = {}
        search_paths = self.get_search_paths('prompt')

        for base_path in search_paths:
            if not base_path.exists():
                continue

            for prompt_file in base_path.glob('*.md'):
                try:
                    content = prompt_file.read_text(encoding='utf-8')
                    sections = self.parse_npl_sections(content)

                    for name, metadata in sections.items():
                        if name not in sources:  # First found wins
                            metadata.source_path = str(prompt_file)
                            sources[name] = metadata
                except Exception:
                    continue

        return sources

    def compare_claude_versions(self, target_path: str = './CLAUDE.md') -> List[Dict[str, Any]]:
        """
        Compare installed CLAUDE.md sections against source prompt versions.
        Returns list of comparison dicts with status.
        """
        comparisons = []

        # Get source prompts
        sources = self.discover_prompt_sources()

        # Get installed sections
        installed = {}
        target = Path(target_path)
        if target.exists():
            try:
                content = target.read_text(encoding='utf-8')
                installed = self.parse_npl_sections(content)
            except Exception:
                pass

        # Compare all known sections
        all_names = set(sources.keys()) | set(installed.keys())

        for name in sorted(all_names):
            source = sources.get(name)
            inst = installed.get(name)

            comparison = {
                'name': name,
                'source_version': source.version if source else None,
                'installed_version': inst.version if inst else None,
                'source_path': source.source_path if source else None,
                'status': 'unknown'
            }

            if source and inst:
                src_v = source.version_tuple()
                inst_v = inst.version_tuple()
                if src_v == inst_v:
                    comparison['status'] = 'current'
                elif src_v > inst_v:
                    comparison['status'] = 'outdated'
                else:
                    comparison['status'] = 'newer'
            elif source and not inst:
                comparison['status'] = 'missing'
            elif inst and not source:
                comparison['status'] = 'extra'

            comparisons.append(comparison)

        return comparisons

    def print_version_table(self, comparisons: List[Dict[str, Any]], as_json: bool = False) -> None:
        """Print version comparison table to stdout."""
        if as_json:
            summary = {
                'current': sum(1 for c in comparisons if c['status'] == 'current'),
                'outdated': sum(1 for c in comparisons if c['status'] == 'outdated'),
                'missing': sum(1 for c in comparisons if c['status'] == 'missing'),
                'extra': sum(1 for c in comparisons if c['status'] == 'extra'),
                'newer': sum(1 for c in comparisons if c['status'] == 'newer'),
            }
            print(json.dumps({'sections': comparisons, 'summary': summary}, indent=2))
            return

        print("NPL Prompt Version Status")
        print("=" * 60)
        print()
        print(f"{'Section':<25} {'Source':<10} {'Installed':<10} {'Status':<10}")
        print("-" * 60)

        for comp in comparisons:
            src_v = comp['source_version'] or '-'
            inst_v = comp['installed_version'] or '-'
            status = comp['status'].upper() if comp['status'] in ('outdated', 'missing') else comp['status']
            print(f"{comp['name']:<25} {src_v:<10} {inst_v:<10} {status:<10}")

        print()
        counts = {}
        for comp in comparisons:
            counts[comp['status']] = counts.get(comp['status'], 0) + 1

        summary_parts = []
        for status in ['outdated', 'missing', 'current', 'extra', 'newer']:
            if counts.get(status, 0) > 0:
                summary_parts.append(f"{counts[status]} {status}")
        print(f"Summary: {', '.join(summary_parts)}")
        print()
        print("Use --update <section1,section2> to update specific sections")
        print("Use --update-all to update all outdated/missing sections")

    def update_claude_sections(self, target_path: str, sections_to_update: List[str],
                               dry_run: bool = False) -> Tuple[int, int]:
        """
        Update specified sections in CLAUDE.md with source versions.
        Returns (updated_count, error_count).
        """
        sources = self.discover_prompt_sources()
        target = Path(target_path)

        # Read existing content
        existing_content = ''
        if target.exists():
            try:
                existing_content = target.read_text(encoding='utf-8')
            except Exception as e:
                print(f"Error reading {target_path}: {e}", file=sys.stderr)
                return (0, 1)

        installed = self.parse_npl_sections(existing_content)

        updated = 0
        errors = 0
        new_content = existing_content

        # Process sections in reverse order of position to maintain offsets
        sections_by_pos = []
        for name in sections_to_update:
            if name in installed:
                sections_by_pos.append((installed[name].start_pos, installed[name].end_pos, name))
        sections_by_pos.sort(reverse=True)

        # Replace existing sections
        for start_pos, end_pos, name in sections_by_pos:
            if name not in sources:
                print(f"Warning: Source not found for {name}", file=sys.stderr)
                errors += 1
                continue

            source = sources[name]
            if dry_run:
                print(f"Would update: {name} ({installed[name].version} -> {source.version})")
            else:
                new_content = new_content[:start_pos] + source.content + new_content[end_pos:]
                print(f"Updated: {name} ({installed[name].version} -> {source.version})", file=sys.stderr)
            updated += 1

        # Add missing sections
        for name in sections_to_update:
            if name not in installed and name in sources:
                source = sources[name]
                if dry_run:
                    print(f"Would add: {name} (v{source.version})")
                else:
                    # Strip leading/trailing * * * from content to avoid double separators
                    section_content = _strip_section_separators(source.content)

                    # Append with separator (join-style)
                    if new_content.strip():
                        new_content = new_content.rstrip() + '\n\n* * *\n\n' + section_content
                    else:
                        new_content = section_content
                    print(f"Added: {name} (v{source.version})", file=sys.stderr)
                updated += 1

        # Write updated content
        if not dry_run and updated > 0:
            try:
                target.write_text(new_content, encoding='utf-8')
            except Exception as e:
                print(f"Error writing {target_path}: {e}", file=sys.stderr)
                return (0, updated + errors)

        return (updated, errors)

    def load_agent(self, agent_name: str) -> Optional[Tuple[str, str]]:
        """Load agent by name from search paths"""
        if agent_name in self.loaded_agents:
            return None

        search_paths = self.get_search_paths('agent')
        for base_path in search_paths:
            agent_file = base_path / f"{agent_name}.md"
            if agent_file.exists():
                try:
                    with open(agent_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    self.loaded_agents.add(agent_name)
                    return (agent_name, f"# agent:{agent_name}:\n{content}")
                except Exception as e:
                    print(f"Error loading agent {agent_file}: {e}", file=sys.stderr)
                    continue
        return None

    def extract_agent_metadata(self, content: str) -> Dict[str, Any]:
        """Extract YAML frontmatter metadata from agent content"""
        metadata = {}

        # Check if content starts with YAML frontmatter
        if content.startswith('---'):
            parts = content.split('---', 2)
            if len(parts) >= 3:
                try:
                    metadata = yaml.safe_load(parts[1])
                    if metadata is None:
                        metadata = {}
                except yaml.YAMLError:
                    metadata = {}

        return metadata

    def discover_agents(self) -> Dict[str, Dict[str, Any]]:
        """Discover all agents across search paths with metadata"""
        agents = {}
        search_paths = self.get_search_paths('agent')

        for base_path in search_paths:
            if not base_path.exists():
                continue

            for agent_file in base_path.glob('*.md'):
                agent_name = agent_file.stem
                if agent_name in agents:
                    continue  # First found wins (priority order)

                try:
                    with open(agent_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    metadata = self.extract_agent_metadata(content)
                    agents[agent_name] = {
                        'path': agent_file,
                        'location': str(agent_file),
                        'metadata': metadata,
                        'content': content
                    }
                except Exception as e:
                    print(f"Error reading agent {agent_file}: {e}", file=sys.stderr)
                    continue

        return agents

    def list_agents(self, verbose: bool = False) -> None:
        """List all available agents with optional metadata"""
        agents = self.discover_agents()

        if not agents:
            print("No agents found in search paths")
            return

        for agent_name in sorted(agents.keys()):
            agent_data = agents[agent_name]
            metadata = agent_data['metadata']

            if verbose:
                # Display detailed metadata
                version = metadata.get('version', 'unknown')
                description = metadata.get('description', 'No description')
                categories = metadata.get('categories', [])
                model = metadata.get('model', 'unknown')

                print(f"{agent_name} (v{version})")
                print(f"  Description: {description}")
                if categories:
                    print(f"  Categories: {', '.join(categories) if isinstance(categories, list) else categories}")
                print(f"  Model: {model}")
                print(f"  Location: {agent_data['location']}")
                print()
            else:
                # Simple listing
                print(agent_name)

    def filter_agents(self, agents: Dict[str, Dict[str, Any]],
                     search: Optional[str] = None,
                     filter_meta: Optional[str] = None,
                     category: Optional[str] = None) -> Dict[str, Dict[str, Any]]:
        """Filter agents based on search criteria"""
        filtered = {}

        # Validate regex pattern once if search is provided
        search_pattern = None
        if search:
            try:
                search_pattern = re.compile(search, re.IGNORECASE)
            except re.error as e:
                print(f"Invalid regex pattern: {e}", file=sys.stderr)
                return {}

        # Validate metadata filter format
        if filter_meta and ':' not in filter_meta:
            print(f"Invalid filter format. Use field:value (got: {filter_meta})", file=sys.stderr)
            return {}

        for agent_name, agent_data in agents.items():
            include = True

            # Body search filter
            if search_pattern and include:
                if not search_pattern.search(agent_data['content']):
                    include = False

            # Metadata filter
            if filter_meta and include:
                field, value = filter_meta.split(':', 1)
                metadata = agent_data['metadata']
                if field not in metadata or str(metadata[field]).lower() != value.lower():
                    include = False

            # Category filter
            if category and include:
                metadata = agent_data['metadata']
                categories = metadata.get('categories', [])
                if isinstance(categories, list):
                    if category.lower() not in [c.lower() for c in categories]:
                        include = False
                elif isinstance(categories, str):
                    if category.lower() not in categories.lower():
                        include = False
                else:
                    include = False

            if include:
                filtered[agent_name] = agent_data

        return filtered

    def search_and_list_agents(self, verbose: bool = False,
                              search: Optional[str] = None,
                              filter_meta: Optional[str] = None,
                              category: Optional[str] = None) -> None:
        """Search and list agents with filtering"""
        agents = self.discover_agents()

        if not agents:
            print("No agents found in search paths")
            return

        # Apply filters
        if search or filter_meta or category:
            agents = self.filter_agents(agents, search, filter_meta, category)

        if not agents:
            print("No agents found matching the specified criteria")
            return

        # List the filtered agents
        for agent_name in sorted(agents.keys()):
            agent_data = agents[agent_name]
            metadata = agent_data['metadata']

            if verbose:
                # Display detailed metadata
                version = metadata.get('version', 'unknown')
                description = metadata.get('description', 'No description')
                categories = metadata.get('categories', [])
                model = metadata.get('model', 'unknown')

                print(f"{agent_name} (v{version})")
                print(f"  Description: {description}")
                if categories:
                    print(f"  Categories: {', '.join(categories) if isinstance(categories, list) else categories}")
                print(f"  Model: {model}")
                print(f"  Location: {agent_data['location']}")
                print()
            else:
                # Simple listing
                print(agent_name)

    def extract_npl_dependencies(self, agent_content: str) -> List[str]:
        """Extract npl-load commands from agent content"""
        dependencies = []

        # Look for npl-load commands in code blocks and text
        npl_load_pattern = r'npl-load\s+c\s+["\']?([^"\'`\n]+)["\']?'
        matches = re.findall(npl_load_pattern, agent_content, re.IGNORECASE)

        for match in matches:
            # Parse comma-separated items, handling quotes
            items = [item.strip().strip('"\'') for item in match.split(',')]
            dependencies.extend(items)

        return dependencies

    def load_npl_components_parallel(self, components: List[str]) -> Dict[str, str]:
        """Load NPL components in parallel using npl-load"""
        component_content = {}

        def load_component(component):
            try:
                # Use the existing npl-load script to load components (without --quiet for now)
                cmd = [sys.executable, str(Path(__file__).parent / 'npl-load'), 'c', component]
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                if result.returncode == 0:
                    # Remove the flag update section from output
                    output = result.stdout
                    # Remove the flag update section
                    if "# Flag Update" in output:
                        lines = output.split('\n')
                        content_lines = []
                        in_flag_section = False
                        for line in lines:
                            if line.strip() == "# Flag Update":
                                in_flag_section = True
                                continue
                            elif in_flag_section and line.strip() == "---":
                                in_flag_section = False
                                continue
                            elif not in_flag_section:
                                content_lines.append(line)
                        output = '\n'.join(content_lines)
                    return component, output
                else:
                    print(f"Warning: Failed to load component {component}: {result.stderr}", file=sys.stderr)
                    return component, ""
            except Exception as e:
                print(f"Warning: Error loading component {component}: {e}", file=sys.stderr)
                return component, ""

        # Load components in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(load_component, comp) for comp in components]
            for future in concurrent.futures.as_completed(futures):
                comp_name, content = future.result()
                if content:
                    component_content[comp_name] = content

        return component_content

    def extract_syntax_elements(self, npl_content: str) -> Set[str]:
        """Extract NPL syntax elements from content"""
        elements = set()

        # Common NPL syntax patterns
        patterns = [
            # Basic syntax
            r'`([^`]+)`',                    # highlight: `term`
            r'\{([^}]+)\}',                  # placeholder: {term}
            r'<([^>]+)>',                    # placeholder: <term>
            r'<<([^>]+)>([^>]+)>',          # qualified: <<qualifier>:term>
            r'\[\.\.\.([^\]]*)\]',          # in-fill: [...] or [...|qualifier]
            r'\[___\]',                      # omission: [___]

            # Special syntax
            r'üéØ\s*([^\n]+)',               # attention: üéØ instruction
            r'\(note:\s*([^\)]+)\)',        # note: (note:[...])
            r'(\w+)\|([^\s\]]+)',           # qualifier: term|qualifier

            # Communication and Routing
            r'üôã\s+([^\n]+)',               # attention-alias: üôã alias
            r'@\{([^}]+)\}',                # direct-message: @{agent}
            r'‚ü™([^‚ü´]+)‚ü´',                   # value-placeholder: ‚ü™content‚ü´

            # Validation and Examples
            r'‚úî\s+([^\n]+)',                # validation: ‚úî positive example
            r'‚ùå\s+([^\n]+)',               # validation: ‚ùå negative example
            r'Ôπç',                          # separator: Ôπç

            # Content Generation
            r'\.\.\.(?![.])',               # inference: ... (not followed by more dots)
            r'\betc\.?\b',                  # inference: etc
            r'`\{~l\|([^`]+)\}`',          # literal-output: `{~l|exact text}`

            # Logic Operators
            r'‚àë\([^)]+\)',                  # logic-operators: ‚àë(expression)
            r'‚à©',                           # logic-operators: intersection
            r'‚à™',                           # logic-operators: union
            r'‚äÜ',                           # logic-operators: subset
            r'‚àà',                           # logic-operators: element of

            # Fences
            r'```(\w+)',                     # fences: ```type
            r'```(\w+-\w+)',                # fences: ```example-type

            # Handlebars
            r'\{\{(\w+)\}\}',               # handlebars: {{var}}
            r'\{\{foreach\s+(\w+)\}\}',     # handlebars: {{foreach item}}
            r'\{\{if\s+([^\}]+)\}\}',       # handlebars: {{if condition}}
            r'\{\{/(\w+)\}\}',              # handlebars: {{/endif}}

            # NPL specific
            r'<npl-(\w+)>',                 # npl tags: <npl-intent>
            r'‚åú([^‚åù]+)‚åù',                   # npl sections: ‚åúcontent‚åù
            r'‚åû([^‚åü]+)‚åü',                   # npl sections: ‚åûcontent‚åü

            # Directives
            r'‚ü™([^:]+):\s*([^‚ü´]+)‚ü´',       # directive: ‚ü™emoji: instruction‚ü´
        ]

        for pattern in patterns:
            matches = re.findall(pattern, npl_content, re.MULTILINE)
            for match in matches:
                if isinstance(match, tuple):
                    elements.update(match)
                else:
                    elements.add(match)

        return elements

    def analyze_npl_syntax_usage(self, content: str, show_matches: bool = False) -> Dict[str, Any]:
        """Analyze NPL syntax elements in any content string and return detailed information"""
        result = {
            'syntax_elements': {},
            'total_elements': 0,
            'element_types': set(),
            'matches': {} if show_matches else None
        }

        # Define pattern categories with their regex patterns
        pattern_categories = {
            'basic_syntax': [
                (r'`([^`]+)`', 'highlight'),
                (r'\{([^}]+)\}', 'placeholder'),
                (r'<([^>]+)>', 'placeholder_angle'),
                (r'<<([^>]+)>([^>]+)>', 'qualified_placeholder'),
                (r'\[\.\.\.([^\]]*)\]', 'in_fill'),
                (r'\[___\]', 'omission'),
            ],
            'special_syntax': [
                (r'üéØ\s*([^\n]+)', 'attention'),
                (r'\(note:\s*([^\)]+)\)', 'note'),
                (r'(\w+)\|([^\s\]]+)', 'qualifier'),
            ],
            'communication': [
                (r'üôã\s+([^\n]+)', 'attention_alias'),
                (r'@\{([^}]+)\}', 'direct_message'),
                (r'‚ü™([^‚ü´]+)‚ü´', 'value_placeholder'),
            ],
            'validation': [
                (r'‚úî\s+([^\n]+)', 'validation_positive'),
                (r'‚ùå\s+([^\n]+)', 'validation_negative'),
                (r'Ôπç', 'separator'),
            ],
            'content_generation': [
                (r'\.\.\.(?![.])', 'inference_dots'),
                (r'\betc\.?\b', 'inference_etc'),
                (r'`\{~l\|([^`]+)\}`', 'literal_output'),
            ],
            'logic_operators': [
                (r'‚àë\([^)]+\)', 'summation'),
                (r'‚à©', 'intersection'),
                (r'‚à™', 'union'),
                (r'‚äÜ', 'subset'),
                (r'‚àà', 'element_of'),
            ],
            'fences': [
                (r'```(\w+)', 'fence_simple'),
                (r'```(\w+-\w+)', 'fence_compound'),
            ],
            'handlebars': [
                (r'\{\{(\w+)\}\}', 'handlebars_var'),
                (r'\{\{foreach\s+(\w+)\}\}', 'handlebars_foreach'),
                (r'\{\{if\s+([^\}]+)\}\}', 'handlebars_if'),
                (r'\{\{/(\w+)\}\}', 'handlebars_end'),
            ],
            'npl_specific': [
                (r'<npl-(\w+)>', 'npl_tag'),
                (r'‚åú([^‚åù]+)‚åù', 'npl_section_start'),
                (r'‚åû([^‚åü]+)‚åü', 'npl_section_end'),
            ],
            'directives': [
                (r'‚ü™([^:]+):\s*([^‚ü´]+)‚ü´', 'directive'),
            ]
        }

        # Analyze each pattern category
        for category, patterns in pattern_categories.items():
            for pattern_regex, element_type in patterns:
                matches = re.findall(pattern_regex, content, re.MULTILINE)
                if matches:
                    count = len(matches)
                    result['syntax_elements'][element_type] = count
                    result['total_elements'] += count
                    result['element_types'].add(category)

                    if show_matches:
                        result['matches'][element_type] = matches

        return result

    def print_syntax_analysis(self, content: str, show_matches: bool = False) -> None:
        """Print a formatted analysis of NPL syntax elements in content"""
        analysis = self.analyze_npl_syntax_usage(content, show_matches)

        print(f"# NPL Syntax Analysis")
        print(f"Total elements found: {analysis['total_elements']}")
        print(f"Element types: {', '.join(sorted(analysis['element_types']))}")
        print()

        if analysis['syntax_elements']:
            print("## Syntax Elements Found:")
            for element_type, count in sorted(analysis['syntax_elements'].items()):
                print(f"  {element_type}: {count}")

                if show_matches and analysis['matches'].get(element_type):
                    matches = analysis['matches'][element_type]
                    for i, match in enumerate(matches[:3]):  # Show first 3 matches
                        if isinstance(match, tuple):
                            match_str = ' | '.join(match)
                        else:
                            match_str = str(match)
                        print(f"    [{i+1}] {match_str}")
                    if len(matches) > 3:
                        print(f"    ... and {len(matches) - 3} more")
                print()
        else:
            print("No NPL syntax elements found.")

    def analyze_agent_npl_usage(self, agent_content: str, available_elements: Set[str]) -> Dict[str, int]:
        """Analyze which NPL elements an agent uses and how frequently"""
        usage = {}

        # Extract all potential syntax elements from agent content
        used_elements = self.extract_syntax_elements(agent_content)

        # Count usage of each element
        for element in used_elements:
            if element in available_elements:
                # Count occurrences more precisely for known elements
                count = len(re.findall(re.escape(element), agent_content, re.IGNORECASE))
                usage[element] = count

        return usage

    def generate_optimized_npl_definition(self, agent_name: str, required_elements: Set[str],
                                        npl_components: Dict[str, str]) -> str:
        """Generate optimized NPL definition file with only required elements"""

        definition_parts = []
        definition_parts.append(f"# NPL Definition for {agent_name}")
        definition_parts.append("")
        definition_parts.append("This file contains only the NPL syntax elements used by this agent.")
        definition_parts.append("")

        # Load base NPL if available
        npl_init = self.load_npl_init()
        if npl_init:
            definition_parts.append("## Core NPL Framework")
            definition_parts.append(npl_init)
            definition_parts.append("")

        # Add relevant component sections
        for component_name, content in npl_components.items():
            if any(element in content for element in required_elements):
                definition_parts.append(f"## Component: {component_name}")
                definition_parts.append(content)
                definition_parts.append("")

        return "\n".join(definition_parts)

    def export_agent_definition(self, agent_name: str) -> Optional[str]:
        """Export agent with optimized NPL documentation"""

        # Load the agent
        result = self.load_agent(agent_name)
        if not result:
            return None

        agent_name, agent_content = result

        # Extract NPL dependencies from agent content
        dependencies = self.extract_npl_dependencies(agent_content)

        # Add common NPL components
        common_components = ['syntax', 'agent', 'fences', 'directive', 'formatting']
        all_components = list(set(dependencies + common_components))

        # Load NPL components in parallel
        npl_components = self.load_npl_components_parallel(all_components)

        # Extract all available syntax elements
        available_elements = set()
        for content in npl_components.values():
            available_elements.update(self.extract_syntax_elements(content))

        # Analyze agent's NPL usage
        usage = self.analyze_agent_npl_usage(agent_content, available_elements)
        required_elements = set(usage.keys())

        # Generate optimized definition
        definition = self.generate_optimized_npl_definition(agent_name, required_elements, npl_components)

        # Create cache file
        cache_dir = Path.home() / '.npl' / 'cache' / 'agent-definitions'
        cache_dir.mkdir(parents=True, exist_ok=True)

        cache_file = cache_dir / f"{agent_name}.npl.md"
        with open(cache_file, 'w', encoding='utf-8') as f:
            f.write(definition)

        # Return combined content
        combined = f"{agent_content}\n\n---\n\n{definition}"
        return combined

    def resolve_schema_path(self, schema_name: str) -> Optional[Path]:
        """Find the first matching schema SQL file."""
        file_name = f"{schema_name}.sql"
        for base in self.get_search_paths('schema'):
            candidate = base / file_name
            if candidate.exists():
                return candidate
        return None

    def load_schema(self, schema_name: str) -> Optional[str]:
        """Load and return the raw schema SQL content (no patches, no wrappers)."""
        p = self.resolve_schema_path(schema_name)
        if not p:
            return None
        try:
            with open(p, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception:
            return None

    def find_npl_init(self) -> Optional[Path]:
        """Find the main npl.md file using the same directory logic as other searches."""
        search_paths = self.get_npl_init_paths()
        for base_path in search_paths:
            npl_file = base_path / 'npl.md'
            if npl_file.exists():
                return npl_file
        return None

    def load_npl_init(self) -> Optional[str]:
        """Load and return the main npl.md file content."""
        npl_file = self.find_npl_init()
        if not npl_file:
            return None
        try:
            with open(npl_file, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception:
            return None

def _parse_skip_list(values) -> Set[str]:
    """Accept '--skip a b c' or '--skip a,b,c'."""
    if not values:
        return set()
    if isinstance(values, str):
        raw = values
    else:
        raw = " ".join(values)
    return {t for t in re.split(r'[,\s]+', raw) if t}


def _strip_section_separators(content: str) -> str:
    """Strip leading/trailing * * * separators for join-style concatenation."""
    content = content.strip()
    if content.startswith('* * *'):
        content = content[5:].lstrip('\n')
    if content.endswith('* * *'):
        content = content[:-5].rstrip('\n')
    return content


def _add_resource_subparser(subparsers, name: str, help_text: str, items_name: str):
    """Factory function to create resource subparsers with standard arguments"""
    sp = subparsers.add_parser(name, help=help_text)
    sp.add_argument(items_name, nargs='*', help=f'{name.title()} to load (supports glob patterns)')
    sp.add_argument('--skip', nargs='*', default=[], help='Skip patterns (wildcards allowed)')
    sp.add_argument('--verbose', action='store_true')
    return sp


# Command configuration: maps command name to (resource_type, items_attr, label)
RESOURCE_COMMANDS = {
    'c': ('component', 'def_items', 'COMPONENT'),
    'm': ('meta', 'meta_items', 'META'),
    's': ('style', 'style_items', 'STYLE'),
    'spec': ('spec', 'spec_items', 'SPEC'),
    'persona': ('persona', 'persona_items', 'PERSONA'),
    'user-persona': ('user-persona', 'user_persona_items', 'USER-PERSONA'),
    'prd': ('prd', 'prd_items', 'PRD'),
    'story': ('story', 'story_items', 'STORY'),
    'prompt': ('prompt', 'prompt_items', 'PROMPT'),
    'workflow': ('workflow', 'workflow_items', 'WORKFLOW'),
}


def main():
    parser = argparse.ArgumentParser(description='Load NPL resources with dependency tracking')
    parser.add_argument('--quiet', action='store_true',
                        help='Only output tracking flags, not content')

    subparsers = parser.add_subparsers(dest='command', required=True)

    # Resource loading subcommands (using factory function)
    _add_resource_subparser(subparsers, 'c', 'Load components', 'def_items')
    _add_resource_subparser(subparsers, 'm', 'Load metadata', 'meta_items')
    _add_resource_subparser(subparsers, 's', 'Load style guides', 'style_items')
    _add_resource_subparser(subparsers, 'spec', 'Load specifications', 'spec_items')
    _add_resource_subparser(subparsers, 'persona', 'Load agent personas', 'persona_items')
    _add_resource_subparser(subparsers, 'user-persona', 'Load user personas (for user story planning)', 'user_persona_items')
    _add_resource_subparser(subparsers, 'prd', 'Load PRD documents', 'prd_items')
    _add_resource_subparser(subparsers, 'story', 'Load user stories', 'story_items')
    _add_resource_subparser(subparsers, 'prompt', 'Load prompts', 'prompt_items')
    _add_resource_subparser(subparsers, 'workflow', 'Load workflows', 'workflow_items')

    # Schema subcommand
    sp_schema = subparsers.add_parser('schema', help='Output the raw SQL for a schema')
    sp_schema.add_argument('schema_name', help='Schema name without .sql (e.g., nimps)')

    # Init subcommand
    sp_init = subparsers.add_parser('init', help='Load the main npl.md file')

    # Agent subcommand
    sp_agent = subparsers.add_parser('agent', help='Load agent definitions')
    sp_agent.add_argument('agent_name', nargs='?', help='Name of agent to load')
    sp_agent.add_argument('--list', action='store_true', help='List all available agents')
    sp_agent.add_argument('--verbose', '-v', action='store_true', help='Show metadata with --list')
    sp_agent.add_argument('--search', help='Search in agent body (regex)')
    sp_agent.add_argument('--filter-meta', help='Filter by metadata field (field:value)')
    sp_agent.add_argument('--category', help='Filter by category')
    sp_agent.add_argument('--definition', action='store_true',
                         help='Get agent with NPL syntax documentation')

    # Syntax subcommand
    sp_syntax = subparsers.add_parser('syntax', help='Analyze NPL syntax elements in content')
    sp_syntax.add_argument('content', nargs='?', help='Content to analyze (use - for stdin)')
    sp_syntax.add_argument('--file', help='File to analyze')
    sp_syntax.add_argument('--matches', action='store_true', help='Show actual matches')

    # init-claude subcommand
    sp_init_claude = subparsers.add_parser('init-claude',
        help='Initialize/append NPL prompts to CLAUDE.md with version tracking')
    sp_init_claude.add_argument('--target', default='./CLAUDE.md',
        help='Target CLAUDE.md path (default: ./CLAUDE.md)')
    sp_init_claude.add_argument('--prompts', nargs='*',
        default=['npl-conventions', 'npl-load-directive', 'npl-scripts', 'sqlite-guide'],
        help='Prompts to append (default: npl-conventions npl-load-directive npl-scripts sqlite-guide)')
    sp_init_claude.add_argument('--update', metavar='SECTIONS',
        help='Update only specified sections (comma-separated names)')
    sp_init_claude.add_argument('--update-all', action='store_true',
        help='Update all outdated/missing sections')
    sp_init_claude.add_argument('--dry-run', action='store_true',
        help='Show what would change without modifying files')
    sp_init_claude.add_argument('--json', action='store_true',
        help='Output version info as JSON')

    args = parser.parse_args()

    # New: handle schema early and exit to ensure no extra output
    if args.command == 'schema':
        loader = NPLLoader()
        content = loader.load_schema(args.schema_name)
        if content is None:
            print(f"Schema not found: {args.schema_name}", file=sys.stderr)
            sys.exit(2)
        # Output only the file contents
        print(content, end='')
        sys.exit(0)

    # New: handle init command early and exit
    if args.command == 'init':
        loader = NPLLoader()
        content = loader.load_npl_init()
        if content is None:
            print("Main npl.md file not found", file=sys.stderr)
            sys.exit(2)
        # Output only the file contents
        print(content, end='')
        sys.exit(0)

    # New: handle agent command early and exit
    if args.command == 'agent':
        loader = NPLLoader()

        if args.list:
            # List all agents with optional filtering
            loader.search_and_list_agents(args.verbose, args.search, args.filter_meta, args.category)
            sys.exit(0)

        if not args.agent_name:
            print("Error: agent_name is required when not using --list", file=sys.stderr)
            sys.exit(1)

        if args.definition:
            # Export agent with NPL documentation
            combined_content = loader.export_agent_definition(args.agent_name)
            if combined_content is None:
                print(f"Agent not found: {args.agent_name}", file=sys.stderr)
                sys.exit(2)
            print(combined_content, end='')
            sys.exit(0)

        result = loader.load_agent(args.agent_name)
        if result is None:
            print(f"Agent not found: {args.agent_name}", file=sys.stderr)
            sys.exit(2)
        agent_name, content = result
        # Output only the agent content
        print(content, end='')
        sys.exit(0)

    # New: handle syntax command early and exit
    if args.command == 'syntax':
        loader = NPLLoader()

        # Get content to analyze
        content_to_analyze = None
        if args.file:
            try:
                with open(args.file, 'r', encoding='utf-8') as f:
                    content_to_analyze = f.read()
            except Exception as e:
                print(f"Error reading file {args.file}: {e}", file=sys.stderr)
                sys.exit(2)
        elif args.content:
            if args.content == '-':
                # Read from stdin
                content_to_analyze = sys.stdin.read()
            else:
                content_to_analyze = args.content
        else:
            print("Error: Must provide content, --file, or - for stdin", file=sys.stderr)
            sys.exit(1)

        # Analyze and print results
        loader.print_syntax_analysis(content_to_analyze, args.matches)
        sys.exit(0)

    # Handle init-claude command early and exit
    if args.command == 'init-claude':
        loader = NPLLoader()
        target = args.target

        # Get version comparisons
        comparisons = loader.compare_claude_versions(target)

        # Check if CLAUDE.md has any versioned sections
        has_versioned_sections = any(c['installed_version'] for c in comparisons)

        # Determine which sections need action
        outdated = [c['name'] for c in comparisons if c['status'] == 'outdated']
        missing = [c['name'] for c in comparisons if c['status'] == 'missing']

        # Handle --update or --update-all flags
        if args.update or args.update_all:
            if args.update:
                sections_to_update = [s.strip() for s in args.update.split(',')]
            else:  # --update-all
                sections_to_update = outdated + missing

            if not sections_to_update:
                print("No sections to update")
                sys.exit(0)

            updated, errors = loader.update_claude_sections(
                target, sections_to_update, dry_run=args.dry_run
            )

            if args.dry_run:
                sys.exit(0)
            elif updated > 0:
                print(f"Updated {updated} section(s) in {target}")
                sys.exit(1)  # Exit 1 = updates applied
            else:
                print(f"No sections updated")
                sys.exit(0 if errors == 0 else 3)

        # If versioned sections exist, show version table and exit
        if has_versioned_sections:
            loader.print_version_table(comparisons, as_json=args.json)

            if outdated or missing:
                sys.exit(2)  # Exit 2 = updates available
            else:
                sys.exit(0)  # All current

        # No versioned sections - fall back to legacy append behavior
        # (for first-time initialization)
        added, skipped = loader.init_claude(target, args.prompts)
        if added > 0:
            print(f"Added {added} prompt(s) to {target}")
            sys.exit(1)  # Exit 1 = updates applied
        if skipped > 0:
            print(f"Skipped {skipped} prompt(s) (already present)")
        if added == 0 and skipped == 0:
            print(f"No prompts added to {target}")
        sys.exit(0)

    loader = NPLLoader()
    loaded_content = []

    # Handle resource commands using dispatch pattern
    if args.command in RESOURCE_COMMANDS:
        resource_type, items_attr, label = RESOURCE_COMMANDS[args.command]
        skip = _parse_skip_list(args.skip)
        # Parse items - support both space-separated args and comma-separated strings
        raw_items = getattr(args, items_attr, [])
        items = []
        for item in raw_items:
            items.extend([i.strip() for i in item.split(',') if i.strip()])
        if items:
            resources = loader.load_resources(items, resource_type, skip)
            for name, content in resources:
                loaded_content.append((label, name, content))

    # Flag output configuration: maps tracking set attr to flag name
    FLAG_CONFIG = [
        ('loaded_components', 'npl.def.loaded'),
        ('loaded_meta', 'npl.meta.loaded'),
        ('loaded_style', 'npl.style.loaded'),
        ('loaded_specs', 'npl.spec.loaded'),
        ('loaded_personas', 'npl.persona.loaded'),
        ('loaded_user_personas', 'npl.user-persona.loaded'),
        ('loaded_prds', 'npl.prd.loaded'),
        ('loaded_stories', 'npl.story.loaded'),
        ('loaded_prompts', 'npl.prompt.loaded'),
        ('loaded_workflows', 'npl.workflow.loaded'),
    ]

    # Check if any content was loaded
    has_loaded = any(getattr(loader, attr, None) for attr, _ in FLAG_CONFIG)

    if has_loaded:
        print("# Flag Update")
        print("\n```üè≥Ô∏è\n")
        for attr, flag_name in FLAG_CONFIG:
            tracking_set = getattr(loader, attr, None)
            if tracking_set:
                print(f"@{flag_name}+=\"{','.join(sorted(tracking_set))}\"")
        print("\n\n```\n")
        print("\n---")

    if not args.quiet:
        for i, (resource_type, name, content) in enumerate(loaded_content):
            print(content)
            if i < len(loaded_content) - 1:
                print("\n\n* * *\n")

if __name__ == '__main__':
    main()